{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d85ecd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# For text preprocessing\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# For Predictions\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fb69bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/gigepogden/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/gigepogden/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/gigepogden/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/gigepogden/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/gigepogden/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/gigepogden/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "\n",
    "# For dataset loading\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "# Load a subset of the 20 Newsgroups dataset\n",
    "categories = ['comp.graphics', 'rec.autos', 'sci.space', 'talk.politics.misc']\n",
    "newsgroups = fetch_20newsgroups(subset='train', categories=categories, random_state=42,remove=('headers','footers','quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae24ee32",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46573f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for easier manipulation\n",
    "df = pd.DataFrame({\n",
    "   'text': newsgroups.data,\n",
    "   'category': [newsgroups.target_names[target] for target in newsgroups.target]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bb30408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (2236, 2)\n",
      "\n",
      "Category distribution:\n",
      "category\n",
      "rec.autos             594\n",
      "sci.space             593\n",
      "comp.graphics         584\n",
      "talk.politics.misc    465\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample document:\n",
      "\n",
      "There was a Science fiction movie sometime ago (I do not remember its \n",
      "name) about a planet in the same orbit of Earth but hidden behind the \n",
      "Sun so it could never be visible from Earth. Turns out that that planet \n",
      "was the exact mirror image of Earth and all its inhabitants looked like \n",
      "the Earthings with the difference that their organs was in the opposite \n",
      "side like the heart was in the right side instead in the left and they \n",
      "would shake hands with the left hand and so on...\n",
      "\n",
      " C.O.EGALON@LAR\n",
      "Category is sci.space\n"
     ]
    }
   ],
   "source": [
    "# Preview the data\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(\"\\nCategory distribution:\")\n",
    "print(df['category'].value_counts())\n",
    "print(\"\\nSample document:\")\n",
    "print(df['text'][750][:500])  # Print first 500 characters of a sample document\n",
    "print('Category is',df['category'][750])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47e84df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "# Helper function for lemmatization with POS tagging\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"\n",
    "    Convert NLTK POS tags to WordNet POS tags\n",
    "    \"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "# Lemmatization function\n",
    "def lemmatize_tokens(tokens):\n",
    "    \"\"\"\n",
    "    Lemmatize tokens with appropriate POS tags\n",
    "    \n",
    "    Parameters:\n",
    "    tokens (list): List of word tokens\n",
    "    \n",
    "    Returns:\n",
    "    list: Lemmatized tokens\n",
    "    \"\"\"\n",
    "    # Tag tokens with parts of speech\n",
    "    tokens_tagged = nltk.pos_tag(tokens)\n",
    "    \n",
    "    # Convert to WordNet POS tags\n",
    "    pos_tokens = [(word[0],get_wordnet_pos(word[1])) for word in tokens_tagged]\n",
    "    \n",
    "    # Lemmatize with POS tags\n",
    "    lemmatized = [lemmatizer.lemmatize(word[0],word[1]) for word in pos_tokens]\n",
    "    \n",
    "    return lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b169697b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_text(text, remove_stopwords=True, lemmatize=True):\n",
    "    # Convert to lowercase\n",
    "    clean_txt = text.lower()\n",
    "    # Remove special characters/numbers\n",
    "    clean_txt = re.sub(r'[^a-zA-Z0-9 ]', '', clean_txt)\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(clean_txt)\n",
    "    filtered_tokens = [token for token in tokens if not token.isnumeric()]\n",
    "    filtered_tokens = [token for token in tokens if len(token) > 0]\n",
    "    # Optional: Remove stopwords\n",
    "    \n",
    "    filtered_tokens = [token for token in filtered_tokens if token not in stop_words]\n",
    "    \n",
    "    # Lemmatize or stem tokens\n",
    "    final_tokens = lemmatize_tokens(filtered_tokens)\n",
    "    # Return the cleaned token list\n",
    "    return final_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1997f47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['processed_tokens'] = df['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ab674b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corpus(list_of_lists):\n",
    "    all_tokens = [token for doc in list_of_lists]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d29689e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('would', 1184), ('get', 1114), ('use', 1036), ('one', 917), ('go', 874), ('make', 831), ('know', 818), ('space', 811), ('think', 795), ('say', 788), ('dont', 779), ('like', 748), ('image', 740), ('people', 720), ('also', 695), ('time', 654), ('work', 640), ('well', 626), ('car', 625), ('program', 623)]\n",
      "[(('mr', 'stephanopoulos'), 341), (('dont', 'know'), 153), (('dont', 'think'), 84), (('im', 'sure'), 76), (('would', 'like'), 76), (('m', 'myers'), 75), (('united', 'state'), 70), (('white', 'house'), 63), (('anyone', 'know'), 62), (('anonymous', 'ftp'), 61), (('health', 'care'), 56), (('administration', 'official'), 53), (('space', 'shuttle'), 52), (('new', 'york'), 52), (('senior', 'administration'), 51), (('stephanopoulos', 'dont'), 49), (('year', 'ago'), 48), (('young', 'people'), 47), (('space', 'station'), 46), (('los', 'angeles'), 43)]\n"
     ]
    }
   ],
   "source": [
    "# Single words\n",
    "counter = Counter([token for doc in df['processed_tokens'] for token in doc])\n",
    "print(counter.most_common(20))\n",
    "\n",
    "# Bigrams\n",
    "from nltk.util import ngrams\n",
    "bigram_counts = Counter(ngrams(df['processed_tokens'].sum(), 2))\n",
    "print(bigram_counts.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eccb069",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_token = df[df['category']=='comp.graphics']['processed_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2f9e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top tokens and bigrams\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "frequent_tokens=positive_freq.most_common(20)\n",
    "tkns = [t[0] for t in frequent_tokens]\n",
    "counts = [t[1] for t in frequent_tokens]\n",
    "plt.bar(tkns,counts)\n",
    "plt.title = \"Top 20 Words in Positive Reviews\"\n",
    "\n",
    "#positive_freq.plot(20, \"Top 20 Words in Positive Reviews\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "frequent_tokens=negative_freq.most_common(20)\n",
    "tkns = [t[0] for t in frequent_tokens]\n",
    "counts = [t[1] for t in frequent_tokens]\n",
    "plt.bar(tkns,counts)\n",
    "plt.title = \"Top 20 Words in Negative Reviews\"\n",
    "#negative_freq.plot(20, title=\"Top 20 Words in Negative Reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf8cdfb",
   "metadata": {},
   "source": [
    "# Classification Model\n",
    "Build the model as follows:\n",
    "\n",
    "1. Split the data first! We cannot do step 2 with the entire corpus because new dat\n",
    "\n",
    "2. Create numerical representation for each observation w/ TF-IDF.  This is much better than bag of words because, as we can see in the earlier section, there are a lot of shared words that do not offer any info about the document's true category.\n",
    "\n",
    "3. Make sure to also isolate the target label (category). \n",
    "\n",
    "4. Train the model.  We will try a simple nearest neighbors approach but quickly fabor a multinomial naive bayes model after doing some research on the matter.\n",
    "\n",
    "Lastly, we will tweak this process by exploring different parameter values for the TFidfVectorizer.\n",
    "\n",
    "A. max_df (default=1.0)\n",
    "When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words). If float in range [0.0, 1.0], the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "min_df (default=1)\n",
    "When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float in range of [0.0, 1.0], the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "max_features (default=None)\n",
    "If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus. Otherwise, all features are used.\n",
    "\n",
    "This parameter is ignored if vocabulary is not None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b341a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['category']\n",
    "X = df['text']\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50a633e",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebca3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TfidfVectorizer object\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the documents\n",
    "tfidf_matrix_train = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Get the feature names (words)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert the TF-IDF matrix to an array\n",
    "tfidf_array_train = tfidf_matrix_train.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6853e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix_test = vectorizer.transform(X_test)\n",
    "tfidf_array_test = tfidf_matrix_test.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d1c6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tfidf_array_train.shape[1])\n",
    "print(tfidf_array_test.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9080891a",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train,y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dd0efba0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3169642857142857"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = knn.predict(X_test)\n",
    "accuracy_score(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "89194196",
   "metadata": {},
   "outputs": [],
   "source": [
    "mb = MultinomialNB()\n",
    "mb.fit(X_train,y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6ace18f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8861607142857143"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = mb.predict(X_test)\n",
    "accuracy_score(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77f46ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = sns.confusion_matrix(y_test, predictions)\n",
    "sns.heatmap(cm, annot=True, fmt='d')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fbda13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
